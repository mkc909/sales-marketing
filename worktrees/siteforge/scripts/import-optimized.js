#!/usr/bin/env node

/**
 * Optimized Data Import Script
 *
 * Performance Optimizations:
 * - Batch size: 10,000 records (10x increase)
 * - Transaction wrapping for bulk inserts
 * - Prepared statements for better performance
 * - Reduced delay between batches: 100ms
 * - Progress tracking and resumability
 * - Automatic rollback on failure
 *
 * Target: 100+ records/second (vs current 9 rec/sec)
 */

import fs from 'fs';
import { parse } from 'csv-parse/sync';
import { execSync } from 'child_process';

// Optimized configuration
const WRANGLER_DB = 'estateflow-db';
const BATCH_SIZE = 10000;  // 10x larger batches
const DELAY_MS = 100;       // Reduced delay
const MAX_RETRIES = 3;
const CHECKPOINT_FILE = '.import-checkpoint.json';

class OptimizedImporter {
  constructor(options = {}) {
    this.dbName = options.dbName || WRANGLER_DB;
    this.batchSize = options.batchSize || BATCH_SIZE;
    this.delay = options.delay || DELAY_MS;
    this.dryRun = options.dryRun || false;
    this.environment = options.environment || 'production';
    this.remote = options.remote !== false;
    this.verbose = options.verbose || false;
  }

  /**
   * Main import function
   */
  async import(filename) {
    const startTime = Date.now();

    console.log('üöÄ Optimized Import Starting...\n');
    console.log(`Configuration:`);
    console.log(`  Database: ${this.dbName}`);
    console.log(`  Batch Size: ${this.batchSize.toLocaleString()} records`);
    console.log(`  Delay: ${this.delay}ms`);
    console.log(`  Environment: ${this.environment}`);
    console.log(`  Remote: ${this.remote}`);
    console.log(`  Dry Run: ${this.dryRun}\n`);

    try {
      // Load and parse CSV
      const records = await this.loadCSV(filename);
      console.log(`üìä Loaded ${records.length.toLocaleString()} records from ${filename}\n`);

      // Check for existing checkpoint
      const checkpoint = this.loadCheckpoint();
      const startIndex = checkpoint?.lastProcessedIndex || 0;

      if (startIndex > 0) {
        console.log(`üìç Resuming from checkpoint: record ${startIndex.toLocaleString()}\n`);
      }

      // Validate sample data
      this.validateRecords(records.slice(0, 10));

      // Process in optimized batches
      const result = await this.processInBatches(records, startIndex);

      // Clear checkpoint on success
      this.clearCheckpoint();

      const totalTime = (Date.now() - startTime) / 1000;
      const rate = result.totalImported / totalTime;

      console.log('\n' + '='.repeat(60));
      console.log('‚úÖ IMPORT COMPLETE!');
      console.log('='.repeat(60));
      console.log(`Total Records: ${result.totalImported.toLocaleString()}`);
      console.log(`Total Time: ${totalTime.toFixed(2)}s`);
      console.log(`Import Rate: ${rate.toFixed(2)} records/second`);
      console.log(`Batches Processed: ${result.batchesProcessed}`);
      console.log(`Errors: ${result.errors}`);

      if (rate >= 100) {
        console.log(`\nüéØ TARGET ACHIEVED: ${rate.toFixed(2)} rec/sec > 100 rec/sec target!`);
      } else {
        console.log(`\n‚ö†Ô∏è  Below target: ${rate.toFixed(2)} rec/sec (target: 100 rec/sec)`);
      }

      return result;

    } catch (error) {
      console.error('\n‚ùå Import failed:', error.message);
      console.log('\nüîß Troubleshooting:');
      console.log('1. Check checkpoint file for resume capability');
      console.log('2. Review error logs above');
      console.log('3. Run with --dry-run to test without executing');
      console.log('4. Use npm run import:rollback to revert changes');
      throw error;
    }
  }

  /**
   * Load and parse CSV file
   */
  async loadCSV(filename) {
    if (!fs.existsSync(filename)) {
      throw new Error(`File not found: ${filename}`);
    }

    const data = fs.readFileSync(filename, 'utf-8');
    const records = parse(data, {
      columns: true,
      skip_empty_lines: true,
      trim: true
    });

    if (records.length === 0) {
      throw new Error('No records found in CSV file');
    }

    return records;
  }

  /**
   * Validate record format
   */
  validateRecords(sampleRecords) {
    console.log('‚úÖ Validating data format...');

    const requiredFields = ['id', 'name', 'license_number', 'email', 'industry', 'state', 'city'];

    for (const record of sampleRecords) {
      for (const field of requiredFields) {
        if (!record[field]) {
          throw new Error(`Missing required field '${field}' in record: ${JSON.stringify(record)}`);
        }
      }

      // Validate email format
      if (!record.email.includes('@')) {
        throw new Error(`Invalid email format: ${record.email}`);
      }
    }

    console.log(`   Validated ${sampleRecords.length} sample records\n`);
  }

  /**
   * Process records in optimized batches with transaction wrapping
   */
  async processInBatches(records, startIndex = 0) {
    const totalBatches = Math.ceil((records.length - startIndex) / this.batchSize);
    let totalImported = 0;
    let batchesProcessed = 0;
    let errors = 0;

    console.log(`üì¶ Processing ${totalBatches} batches...\n`);

    for (let i = startIndex; i < records.length; i += this.batchSize) {
      const batchNumber = Math.floor(i / this.batchSize) + 1;
      const batch = records.slice(i, i + this.batchSize);

      const batchStart = Date.now();

      try {
        // Process batch with transaction wrapping
        const imported = await this.processBatch(batch, batchNumber, totalBatches);

        totalImported += imported;
        batchesProcessed++;

        const batchTime = (Date.now() - batchStart) / 1000;
        const batchRate = imported / batchTime;

        console.log(`   ‚úÖ Batch ${batchNumber}/${totalBatches}: ${imported.toLocaleString()} records in ${batchTime.toFixed(2)}s (${batchRate.toFixed(2)} rec/sec)`);

        // Save checkpoint
        this.saveCheckpoint({
          lastProcessedIndex: i + this.batchSize,
          totalImported,
          batchesProcessed,
          timestamp: new Date().toISOString()
        });

        // Delay between batches to avoid overwhelming the database
        if (i + this.batchSize < records.length) {
          await this.sleep(this.delay);
        }

      } catch (error) {
        console.error(`   ‚ùå Batch ${batchNumber} failed:`, error.message);
        errors++;

        // Retry logic
        let retryCount = 0;
        let success = false;

        while (retryCount < MAX_RETRIES && !success) {
          retryCount++;
          console.log(`   üîÑ Retry ${retryCount}/${MAX_RETRIES}...`);

          try {
            await this.sleep(1000 * retryCount); // Exponential backoff
            const imported = await this.processBatch(batch, batchNumber, totalBatches);
            totalImported += imported;
            batchesProcessed++;
            success = true;
            console.log(`   ‚úÖ Retry successful`);
          } catch (retryError) {
            if (retryCount === MAX_RETRIES) {
              throw new Error(`Batch ${batchNumber} failed after ${MAX_RETRIES} retries: ${retryError.message}`);
            }
          }
        }
      }
    }

    return { totalImported, batchesProcessed, errors };
  }

  /**
   * Process a single batch with transaction wrapping
   */
  async processBatch(records, batchNumber, totalBatches) {
    if (this.dryRun) {
      console.log(`   [DRY RUN] Would import ${records.length} records`);
      return records.length;
    }

    // Generate SQL with transaction wrapping for atomicity
    const sql = this.generateBulkInsertSQL(records);

    // Write to temp file
    const tempFile = `batch-${batchNumber}.sql`;
    fs.writeFileSync(tempFile, sql);

    try {
      // Execute with wrangler
      const envFlag = this.environment ? `--env=${this.environment}` : '';
      const remoteFlag = this.remote ? '--remote' : '--local';

      const command = `wrangler d1 execute ${this.dbName} ${envFlag} ${remoteFlag} --file=${tempFile}`;

      if (this.verbose) {
        console.log(`   Executing: ${command}`);
      }

      execSync(command, {
        encoding: 'utf-8',
        stdio: this.verbose ? 'inherit' : 'pipe'
      });

      // Clean up temp file
      fs.unlinkSync(tempFile);

      return records.length;

    } catch (error) {
      // Clean up temp file on error
      if (fs.existsSync(tempFile)) {
        fs.unlinkSync(tempFile);
      }
      throw error;
    }
  }

  /**
   * Generate bulk insert SQL with transaction wrapping
   */
  generateBulkInsertSQL(records) {
    const values = records.map(record => {
      // Generate slug from name
      const slug = this.generateSlug(record.name, record.id);

      // Escape values for SQL
      const escapedValues = [
        this.escapeSQL(record.id),
        this.escapeSQL(slug),
        this.escapeSQL(record.first_name || record.name.split(' ')[0]),
        this.escapeSQL(record.last_name || record.name.split(' ').slice(1).join(' ')),
        this.escapeSQL(record.industry),
        this.escapeSQL(record.profession || 'agent'),
        this.escapeSQL(record.company || 'Independent'),
        this.escapeSQL(record.license_number),
        this.escapeSQL(record.license_state || record.state),
        this.escapeSQL(record.years_experience || 5),
        this.escapeSQL(record.specializations || '[]'),
        this.escapeSQL(record.certifications || '[]'),
        this.escapeSQL(record.phone),
        this.escapeSQL(record.email),
        this.escapeSQL(record.website || ''),
        this.escapeSQL(record.address || ''),
        this.escapeSQL(record.city),
        this.escapeSQL(record.state),
        this.escapeSQL(record.zip_code || ''),
        this.escapeSQL(record.county || ''),
        this.escapeSQL(record.service_regions || '[]'),
        this.escapeSQL(record.bio || ''),
        this.escapeSQL(record.rating || 4.5),
        this.escapeSQL(record.review_count || 0),
        this.escapeSQL(record.languages || '["English"]'),
        this.escapeSQL(record.verified !== false ? 1 : 0),
        this.escapeSQL(record.featured || 0),
        this.escapeSQL(record.active !== false ? 1 : 0)
      ];

      return `(${escapedValues.join(', ')}, datetime('now'), datetime('now'))`;
    }).join(',\n    ');

    // Wrap in transaction for atomicity and performance
    return `
BEGIN TRANSACTION;

INSERT INTO professionals (
  id, slug, first_name, last_name, industry, profession,
  company, license_number, license_state, years_experience,
  specializations, certifications, phone, email, website,
  address, city, state, zip_code, county, service_regions,
  bio, rating, review_count, languages, verified, featured,
  active, created_at, updated_at
) VALUES
    ${values};

COMMIT;
`;
  }

  /**
   * Generate URL-friendly slug
   */
  generateSlug(name, id) {
    return name
      .toLowerCase()
      .replace(/[^a-z0-9\s]/g, '')
      .replace(/\s+/g, '-')
      .replace(/^-+|-+$/g, '') + `-${id}`;
  }

  /**
   * Escape SQL values
   */
  escapeSQL(value) {
    if (value === null || value === undefined) return 'NULL';
    if (typeof value === 'number') return value;
    if (typeof value === 'boolean') return value ? 1 : 0;
    if (Array.isArray(value) || typeof value === 'object') {
      return `'${JSON.stringify(value).replace(/'/g, "''")}'`;
    }
    return `'${String(value).replace(/'/g, "''")}'`;
  }

  /**
   * Checkpoint management for resumability
   */
  saveCheckpoint(data) {
    fs.writeFileSync(CHECKPOINT_FILE, JSON.stringify(data, null, 2));
  }

  loadCheckpoint() {
    if (fs.existsSync(CHECKPOINT_FILE)) {
      return JSON.parse(fs.readFileSync(CHECKPOINT_FILE, 'utf-8'));
    }
    return null;
  }

  clearCheckpoint() {
    if (fs.existsSync(CHECKPOINT_FILE)) {
      fs.unlinkSync(CHECKPOINT_FILE);
    }
  }

  /**
   * Sleep utility
   */
  sleep(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}

// ============================================================================
// CLI Interface
// ============================================================================

async function main() {
  const args = process.argv.slice(2);

  // Parse command line arguments
  const options = {
    filename: args.find(arg => !arg.startsWith('--')) || 'data/test-10.sql',
    batchSize: parseInt(args.find(arg => arg.startsWith('--batch-size='))?.split('=')[1]) || BATCH_SIZE,
    delay: parseInt(args.find(arg => arg.startsWith('--delay='))?.split('=')[1]) || DELAY_MS,
    dryRun: args.includes('--dry-run'),
    environment: args.find(arg => arg.startsWith('--env='))?.split('=')[1] || 'production',
    remote: !args.includes('--local'),
    verbose: args.includes('--verbose') || args.includes('-v')
  };

  if (args.includes('--help') || args.includes('-h')) {
    console.log(`
Optimized Data Import Script

Usage:
  node import-optimized.js [filename] [options]

Options:
  --batch-size=N     Batch size (default: ${BATCH_SIZE})
  --delay=N          Delay between batches in ms (default: ${DELAY_MS})
  --env=ENV          Environment (default: production)
  --local            Use local database instead of remote
  --dry-run          Test mode without executing
  --verbose, -v      Verbose output
  --help, -h         Show this help

Examples:
  node import-optimized.js data/test-10.sql
  node import-optimized.js data/large-10000.sql --batch-size=5000
  node import-optimized.js data/test-10.sql --dry-run
  node import-optimized.js data/large-10000.sql --verbose
    `);
    process.exit(0);
  }

  const importer = new OptimizedImporter(options);

  try {
    await importer.import(options.filename);
    process.exit(0);
  } catch (error) {
    console.error('\nüí• Fatal Error:', error.message);
    process.exit(1);
  }
}

// Run if called directly
if (import.meta.url === `file://${process.argv[1]}`) {
  main();
}

export { OptimizedImporter };
